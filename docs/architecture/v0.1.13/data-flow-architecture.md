# TradingAgents æ•°æ®æµæ¶æ„

## æ¦‚è¿°

TradingAgents é‡‡ç”¨å¤šå±‚æ¬¡æ•°æ®æµæ¶æ„ï¼Œæ”¯æŒä¸­å›½Aè‚¡ã€æ¸¯è‚¡å’Œç¾è‚¡çš„å…¨é¢æ•°æ®è·å–å’Œå¤„ç†ã€‚ç³»ç»Ÿé€šè¿‡ç»Ÿä¸€çš„æ•°æ®æ¥å£ã€æ™ºèƒ½çš„æ•°æ®æºç®¡ç†å’Œé«˜æ•ˆçš„ç¼“å­˜æœºåˆ¶ï¼Œä¸ºæ™ºèƒ½ä½“æä¾›é«˜è´¨é‡çš„é‡‘èæ•°æ®æœåŠ¡ã€‚

## ğŸ—ï¸ æ•°æ®æµæ¶æ„è®¾è®¡

### æ¶æ„å±‚æ¬¡å›¾

```mermaid
graph TB
    subgraph "å¤–éƒ¨æ•°æ®æºå±‚ (External Data Sources)"
        subgraph "ä¸­å›½å¸‚åœºæ•°æ®"
            TUSHARE[Tushareä¸“ä¸šæ•°æ®]
            AKSHARE[AKShareå¼€æºæ•°æ®]
            BAOSTOCK[BaoStockå†å²æ•°æ®]
            TDX[TDXé€šè¾¾ä¿¡æ•°æ® - å·²å¼ƒç”¨]
        end
        
        subgraph "å›½é™…å¸‚åœºæ•°æ®"
            YFINANCE[Yahoo Finance]
            FINNHUB[FinnHub]
            SIMFIN[SimFin]
        end
        
        subgraph "æ–°é—»æƒ…ç»ªæ•°æ®"
            REDDIT[Redditç¤¾äº¤åª’ä½“]
            GOOGLENEWS[Googleæ–°é—»]
            CHINESE_SOCIAL[ä¸­å›½ç¤¾äº¤åª’ä½“]
        end
    end
    
    subgraph "æ•°æ®è·å–å±‚ (Data Acquisition Layer)"
        DSM[æ•°æ®æºç®¡ç†å™¨]
        ADAPTERS[æ•°æ®é€‚é…å™¨]
        API_MGR[APIç®¡ç†å™¨]
    end
    
    subgraph "æ•°æ®å¤„ç†å±‚ (Data Processing Layer)"
        CLEANER[æ•°æ®æ¸…æ´—]
        TRANSFORMER[æ•°æ®è½¬æ¢]
        VALIDATOR[æ•°æ®éªŒè¯]
        QUALITY[è´¨é‡æ§åˆ¶]
    end
    
    subgraph "æ•°æ®å­˜å‚¨å±‚ (Data Storage Layer)"
        CACHE[ç¼“å­˜ç³»ç»Ÿ]
        FILES[æ–‡ä»¶å­˜å‚¨]
        CONFIG[é…ç½®ç®¡ç†]
    end
    
    subgraph "æ•°æ®åˆ†å‘å±‚ (Data Distribution Layer)"
        INTERFACE[ç»Ÿä¸€æ•°æ®æ¥å£]
        ROUTER[æ•°æ®è·¯ç”±å™¨]
        FORMATTER[æ ¼å¼åŒ–å™¨]
    end
    
    subgraph "å·¥å…·é›†æˆå±‚ (Tool Integration Layer)"
        TOOLKIT[Toolkitå·¥å…·åŒ…]
        UNIFIED_TOOLS[ç»Ÿä¸€å·¥å…·æ¥å£]
        STOCK_UTILS[è‚¡ç¥¨å·¥å…·]
    end
    
    subgraph "æ™ºèƒ½ä½“æ¶ˆè´¹å±‚ (Agent Consumption Layer)"
        ANALYSTS[åˆ†æå¸ˆæ™ºèƒ½ä½“]
        RESEARCHERS[ç ”ç©¶å‘˜æ™ºèƒ½ä½“]
        TRADER[äº¤æ˜“å‘˜æ™ºèƒ½ä½“]
        MANAGERS[ç®¡ç†å±‚æ™ºèƒ½ä½“]
    end
    
    %% æ•°æ®æµå‘
    TUSHARE --> DSM
    AKSHARE --> DSM
    BAOSTOCK --> DSM
    TDX --> DSM
    YFINANCE --> ADAPTERS
    FINNHUB --> ADAPTERS
    SIMFIN --> ADAPTERS
    REDDIT --> API_MGR
    GOOGLENEWS --> API_MGR
    CHINESE_SOCIAL --> API_MGR
    
    DSM --> CLEANER
    ADAPTERS --> CLEANER
    API_MGR --> CLEANER
    
    CLEANER --> TRANSFORMER
    TRANSFORMER --> VALIDATOR
    VALIDATOR --> QUALITY
    
    QUALITY --> CACHE
    QUALITY --> FILES
    QUALITY --> CONFIG
    
    CACHE --> INTERFACE
    FILES --> INTERFACE
    CONFIG --> INTERFACE
    
    INTERFACE --> ROUTER
    ROUTER --> FORMATTER
    
    FORMATTER --> TOOLKIT
    TOOLKIT --> UNIFIED_TOOLS
    UNIFIED_TOOLS --> STOCK_UTILS
    
    STOCK_UTILS --> ANALYSTS
    STOCK_UTILS --> RESEARCHERS
    STOCK_UTILS --> TRADER
    STOCK_UTILS --> MANAGERS
    
    %% æ ·å¼å®šä¹‰
    classDef sourceLayer fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef acquisitionLayer fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef processingLayer fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef storageLayer fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef distributionLayer fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef toolLayer fill:#e0f2f1,stroke:#00695c,stroke-width:2px
    classDef agentLayer fill:#f1f8e9,stroke:#558b2f,stroke-width:2px
    
    class TUSHARE,AKSHARE,BAOSTOCK,TDX,YFINANCE,FINNHUB,SIMFIN,REDDIT,GOOGLENEWS,CHINESE_SOCIAL sourceLayer
    class DSM,ADAPTERS,API_MGR acquisitionLayer
    class CLEANER,TRANSFORMER,VALIDATOR,QUALITY processingLayer
    class CACHE,FILES,CONFIG storageLayer
    class INTERFACE,ROUTER,FORMATTER distributionLayer
    class TOOLKIT,UNIFIED_TOOLS,STOCK_UTILS toolLayer
    class ANALYSTS,RESEARCHERS,TRADER,MANAGERS agentLayer
```

## ğŸ“Š å„å±‚æ¬¡è¯¦ç»†è¯´æ˜

### 1. å¤–éƒ¨æ•°æ®æºå±‚ (External Data Sources)

#### ä¸­å›½å¸‚åœºæ•°æ®æº

##### Tushare ä¸“ä¸šæ•°æ®æº (æ¨è)
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/tushare_utils.py`

```python
import tushare as ts
from tradingagents.utils.logging_manager import get_logger

class TushareProvider:
    """Tushareæ•°æ®æä¾›å•†"""
    
    def __init__(self):
        self.token = os.getenv('TUSHARE_TOKEN')
        if self.token:
            ts.set_token(self.token)
            self.pro = ts.pro_api()
        else:
            raise ValueError("TUSHARE_TOKENç¯å¢ƒå˜é‡æœªè®¾ç½®")
    
    def get_stock_data(self, ts_code: str, start_date: str, end_date: str):
        """è·å–è‚¡ç¥¨å†å²æ•°æ®"""
        try:
            df = self.pro.daily(
                ts_code=ts_code,
                start_date=start_date.replace('-', ''),
                end_date=end_date.replace('-', '')
            )
            return df
        except Exception as e:
            logger.error(f"Tushareæ•°æ®è·å–å¤±è´¥: {e}")
            return None
    
    def get_stock_basic(self, ts_code: str):
        """è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯"""
        try:
            df = self.pro.stock_basic(
                ts_code=ts_code,
                fields='ts_code,symbol,name,area,industry,market,list_date'
            )
            return df
        except Exception as e:
            logger.error(f"TushareåŸºæœ¬ä¿¡æ¯è·å–å¤±è´¥: {e}")
            return None
```

##### AKShare å¼€æºæ•°æ®æº (å¤‡ç”¨)
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/akshare_utils.py`

```python
import akshare as ak
import pandas as pd
from typing import Optional, Dict, Any

def get_akshare_provider():
    """è·å–AKShareæ•°æ®æä¾›å•†å®ä¾‹"""
    return AKShareProvider()

class AKShareProvider:
    """AKShareæ•°æ®æä¾›å•†"""
    
    def __init__(self):
        self.logger = get_logger('agents')
    
    def get_stock_zh_a_hist(self, symbol: str, period: str = "daily", 
                           start_date: str = None, end_date: str = None):
        """è·å–Aè‚¡å†å²æ•°æ®"""
        try:
            df = ak.stock_zh_a_hist(
                symbol=symbol,
                period=period,
                start_date=start_date,
                end_date=end_date,
                adjust="qfq"  # å‰å¤æƒ
            )
            return df
        except Exception as e:
            self.logger.error(f"AKShare Aè‚¡æ•°æ®è·å–å¤±è´¥: {e}")
            return None
    
    def get_hk_stock_data_akshare(self, symbol: str, period: str = "daily"):
        """è·å–æ¸¯è‚¡æ•°æ®"""
        try:
            # æ¸¯è‚¡ä»£ç æ ¼å¼è½¬æ¢
            if not symbol.startswith('0') and len(symbol) <= 5:
                symbol = symbol.zfill(5)
            
            df = ak.stock_hk_hist(
                symbol=symbol,
                period=period,
                adjust="qfq"
            )
            return df
        except Exception as e:
            self.logger.error(f"AKShareæ¸¯è‚¡æ•°æ®è·å–å¤±è´¥: {e}")
            return None
    
    def get_hk_stock_info_akshare(self, symbol: str):
        """è·å–æ¸¯è‚¡åŸºæœ¬ä¿¡æ¯"""
        try:
            df = ak.stock_hk_spot_em()
            if not df.empty:
                # æŸ¥æ‰¾åŒ¹é…çš„è‚¡ç¥¨
                matched = df[df['ä»£ç '].str.contains(symbol, na=False)]
                return matched
            return None
        except Exception as e:
            self.logger.error(f"AKShareæ¸¯è‚¡ä¿¡æ¯è·å–å¤±è´¥: {e}")
            return None
```

##### BaoStock å†å²æ•°æ®æº (å¤‡ç”¨)
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/baostock_utils.py`

```python
import baostock as bs
import pandas as pd

class BaoStockProvider:
    """BaoStockæ•°æ®æä¾›å•†"""
    
    def __init__(self):
        self.logger = get_logger('agents')
        self.login_result = bs.login()
        if self.login_result.error_code != '0':
            self.logger.error(f"BaoStockç™»å½•å¤±è´¥: {self.login_result.error_msg}")
    
    def get_stock_data(self, code: str, start_date: str, end_date: str):
        """è·å–è‚¡ç¥¨å†å²æ•°æ®"""
        try:
            rs = bs.query_history_k_data_plus(
                code,
                "date,code,open,high,low,close,preclose,volume,amount,adjustflag,turn,tradestatus,pctChg,isST",
                start_date=start_date,
                end_date=end_date,
                frequency="d",
                adjustflag="3"  # å‰å¤æƒ
            )
            
            data_list = []
            while (rs.error_code == '0') & rs.next():
                data_list.append(rs.get_row_data())
            
            df = pd.DataFrame(data_list, columns=rs.fields)
            return df
        except Exception as e:
            self.logger.error(f"BaoStockæ•°æ®è·å–å¤±è´¥: {e}")
            return None
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç™»å‡ºBaoStock"""
        bs.logout()
```

#### å›½é™…å¸‚åœºæ•°æ®æº

##### Yahoo Finance
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/yfin_utils.py`

```python
import yfinance as yf
import pandas as pd
from typing import Optional

def get_yahoo_finance_data(ticker: str, period: str = "1y", 
                          start_date: str = None, end_date: str = None):
    """è·å–Yahoo Financeæ•°æ®
    
    Args:
        ticker: è‚¡ç¥¨ä»£ç 
        period: æ—¶é—´å‘¨æœŸ (1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max)
        start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
    
    Returns:
        DataFrame: è‚¡ç¥¨æ•°æ®
    """
    try:
        stock = yf.Ticker(ticker)
        
        if start_date and end_date:
            data = stock.history(start=start_date, end=end_date)
        else:
            data = stock.history(period=period)
        
        if data.empty:
            logger.warning(f"Yahoo Financeæœªæ‰¾åˆ°{ticker}çš„æ•°æ®")
            return None
        
        return data
    except Exception as e:
        logger.error(f"Yahoo Financeæ•°æ®è·å–å¤±è´¥: {e}")
        return None

def get_stock_info_yahoo(ticker: str):
    """è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯"""
    try:
        stock = yf.Ticker(ticker)
        info = stock.info
        return info
    except Exception as e:
        logger.error(f"Yahoo Financeä¿¡æ¯è·å–å¤±è´¥: {e}")
        return None
```

##### FinnHub æ–°é—»å’ŒåŸºæœ¬é¢æ•°æ®
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/finnhub_utils.py`

```python
from datetime import datetime, relativedelta
import json
import os

def get_data_in_range(ticker: str, start_date: str, end_date: str, 
                     data_type: str, data_dir: str):
    """ä»ç¼“å­˜ä¸­è·å–æŒ‡å®šæ—¶é—´èŒƒå›´çš„æ•°æ®
    
    Args:
        ticker: è‚¡ç¥¨ä»£ç 
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        data_type: æ•°æ®ç±»å‹ (news_data, insider_senti, insider_trans)
        data_dir: æ•°æ®ç›®å½•
    
    Returns:
        dict: æ•°æ®å­—å…¸
    """
    try:
        file_path = os.path.join(data_dir, f"{ticker}_{data_type}.json")
        
        if not os.path.exists(file_path):
            logger.warning(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return {}
        
        with open(file_path, 'r', encoding='utf-8') as f:
            all_data = json.load(f)
        
        # è¿‡æ»¤æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
        filtered_data = {}
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        
        for date_str, data in all_data.items():
            try:
                data_dt = datetime.strptime(date_str, "%Y-%m-%d")
                if start_dt <= data_dt <= end_dt:
                    filtered_data[date_str] = data
            except ValueError:
                continue
        
        return filtered_data
    except Exception as e:
        logger.error(f"æ•°æ®è·å–å¤±è´¥: {e}")
        return {}
```

#### æ–°é—»æƒ…ç»ªæ•°æ®æº

##### Reddit ç¤¾äº¤åª’ä½“
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/reddit_utils.py`

```python
import praw
import os
from typing import List, Dict

def fetch_top_from_category(subreddit: str, category: str = "hot", 
                           limit: int = 10) -> List[Dict]:
    """ä»Redditè·å–çƒ­é—¨å¸–å­
    
    Args:
        subreddit: å­ç‰ˆå—åç§°
        category: åˆ†ç±» (hot, new, top)
        limit: è·å–æ•°é‡é™åˆ¶
    
    Returns:
        List[Dict]: å¸–å­åˆ—è¡¨
    """
    try:
        reddit = praw.Reddit(
            client_id=os.getenv('REDDIT_CLIENT_ID'),
            client_secret=os.getenv('REDDIT_CLIENT_SECRET'),
            user_agent='TradingAgents/1.0'
        )
        
        subreddit_obj = reddit.subreddit(subreddit)
        
        if category == "hot":
            posts = subreddit_obj.hot(limit=limit)
        elif category == "new":
            posts = subreddit_obj.new(limit=limit)
        elif category == "top":
            posts = subreddit_obj.top(limit=limit)
        else:
            posts = subreddit_obj.hot(limit=limit)
        
        results = []
        for post in posts:
            results.append({
                'title': post.title,
                'score': post.score,
                'url': post.url,
                'created_utc': post.created_utc,
                'num_comments': post.num_comments,
                'selftext': post.selftext[:500] if post.selftext else ''
            })
        
        return results
    except Exception as e:
        logger.error(f"Redditæ•°æ®è·å–å¤±è´¥: {e}")
        return []
```

##### ä¸­å›½ç¤¾äº¤åª’ä½“æƒ…ç»ª
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/chinese_finance_utils.py`

```python
def get_chinese_social_sentiment(ticker: str, platform: str = "weibo"):
    """è·å–ä¸­å›½ç¤¾äº¤åª’ä½“æƒ…ç»ªæ•°æ®
    
    Args:
        ticker: è‚¡ç¥¨ä»£ç 
        platform: å¹³å°åç§° (weibo, xueqiu, eastmoney)
    
    Returns:
        str: æƒ…ç»ªåˆ†ææŠ¥å‘Š
    """
    try:
        # è¿™é‡Œå¯ä»¥é›†æˆå¾®åšã€é›ªçƒã€ä¸œæ–¹è´¢å¯Œç­‰å¹³å°çš„API
        # ç›®å‰è¿”å›æ¨¡æ‹Ÿæ•°æ®
        sentiment_data = {
            'positive_ratio': 0.65,
            'negative_ratio': 0.25,
            'neutral_ratio': 0.10,
            'total_mentions': 1250,
            'trending_keywords': ['ä¸Šæ¶¨', 'åˆ©å¥½', 'ä¸šç»©', 'å¢é•¿']
        }
        
        report = f"""## {ticker} ä¸­å›½ç¤¾äº¤åª’ä½“æƒ…ç»ªåˆ†æ
        
**å¹³å°**: {platform}
**æ€»æåŠæ•°**: {sentiment_data['total_mentions']}
**æƒ…ç»ªåˆ†å¸ƒ**:
- ç§¯æ: {sentiment_data['positive_ratio']:.1%}
- æ¶ˆæ: {sentiment_data['negative_ratio']:.1%}
- ä¸­æ€§: {sentiment_data['neutral_ratio']:.1%}

**çƒ­é—¨å…³é”®è¯**: {', '.join(sentiment_data['trending_keywords'])}
        """
        
        return report
    except Exception as e:
        logger.error(f"ä¸­å›½ç¤¾äº¤åª’ä½“æƒ…ç»ªè·å–å¤±è´¥: {e}")
        return f"ä¸­å›½ç¤¾äº¤åª’ä½“æƒ…ç»ªæ•°æ®è·å–å¤±è´¥: {str(e)}"
```

### 2. æ•°æ®è·å–å±‚ (Data Acquisition Layer)

#### æ•°æ®æºç®¡ç†å™¨
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/data_source_manager.py`

```python
from enum import Enum
from typing import List, Optional

class ChinaDataSource(Enum):
    """ä¸­å›½è‚¡ç¥¨æ•°æ®æºæšä¸¾"""
    TUSHARE = "tushare"
    AKSHARE = "akshare"
    BAOSTOCK = "baostock"
    TDX = "tdx"  # å·²å¼ƒç”¨

class DataSourceManager:
    """æ•°æ®æºç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®æºç®¡ç†å™¨"""
        self.default_source = self._get_default_source()
        self.available_sources = self._check_available_sources()
        self.current_source = self.default_source
        
        logger.info(f"ğŸ“Š æ•°æ®æºç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   é»˜è®¤æ•°æ®æº: {self.default_source.value}")
        logger.info(f"   å¯ç”¨æ•°æ®æº: {[s.value for s in self.available_sources]}")
    
    def _get_default_source(self) -> ChinaDataSource:
        """è·å–é»˜è®¤æ•°æ®æº"""
        default = os.getenv('DEFAULT_CHINA_DATA_SOURCE', 'tushare').lower()
        
        try:
            return ChinaDataSource(default)
        except ValueError:
            logger.warning(f"âš ï¸ æ— æ•ˆçš„é»˜è®¤æ•°æ®æº: {default}ï¼Œä½¿ç”¨Tushare")
            return ChinaDataSource.TUSHARE
    
    def _check_available_sources(self) -> List[ChinaDataSource]:
        """æ£€æŸ¥å¯ç”¨çš„æ•°æ®æº"""
        available = []
        
        # æ£€æŸ¥Tushare
        try:
            import tushare as ts
            token = os.getenv('TUSHARE_TOKEN')
            if token:
                available.append(ChinaDataSource.TUSHARE)
                logger.info("âœ… Tushareæ•°æ®æºå¯ç”¨")
            else:
                logger.warning("âš ï¸ Tushareæ•°æ®æºä¸å¯ç”¨: æœªè®¾ç½®TUSHARE_TOKEN")
        except ImportError:
            logger.warning("âš ï¸ Tushareæ•°æ®æºä¸å¯ç”¨: åº“æœªå®‰è£…")
        
        # æ£€æŸ¥AKShare
        try:
            import akshare as ak
            available.append(ChinaDataSource.AKSHARE)
            logger.info("âœ… AKShareæ•°æ®æºå¯ç”¨")
        except ImportError:
            logger.warning("âš ï¸ AKShareæ•°æ®æºä¸å¯ç”¨: åº“æœªå®‰è£…")
        
        # æ£€æŸ¥BaoStock
        try:
            import baostock as bs
            available.append(ChinaDataSource.BAOSTOCK)
            logger.info("âœ… BaoStockæ•°æ®æºå¯ç”¨")
        except ImportError:
            logger.warning("âš ï¸ BaoStockæ•°æ®æºä¸å¯ç”¨: åº“æœªå®‰è£…")
        
        # æ£€æŸ¥TDX (å·²å¼ƒç”¨)
        try:
            import pytdx
            available.append(ChinaDataSource.TDX)
            logger.warning("âš ï¸ TDXæ•°æ®æºå¯ç”¨ä½†å·²å¼ƒç”¨ï¼Œå»ºè®®è¿ç§»åˆ°Tushare")
        except ImportError:
            logger.info("â„¹ï¸ TDXæ•°æ®æºä¸å¯ç”¨: åº“æœªå®‰è£…")
        
        return available
    
    def switch_source(self, source_name: str) -> str:
        """åˆ‡æ¢æ•°æ®æº
        
        Args:
            source_name: æ•°æ®æºåç§°
        
        Returns:
            str: åˆ‡æ¢ç»“æœæ¶ˆæ¯
        """
        try:
            new_source = ChinaDataSource(source_name.lower())
            
            if new_source in self.available_sources:
                self.current_source = new_source
                logger.info(f"âœ… æ•°æ®æºå·²åˆ‡æ¢åˆ°: {new_source.value}")
                return f"âœ… æ•°æ®æºå·²æˆåŠŸåˆ‡æ¢åˆ°: {new_source.value}"
            else:
                logger.warning(f"âš ï¸ æ•°æ®æº{new_source.value}ä¸å¯ç”¨")
                return f"âš ï¸ æ•°æ®æº{new_source.value}ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥å®‰è£…å’Œé…ç½®"
        except ValueError:
            logger.error(f"âŒ æ— æ•ˆçš„æ•°æ®æºåç§°: {source_name}")
            return f"âŒ æ— æ•ˆçš„æ•°æ®æºåç§°: {source_name}"
    
    def get_current_source(self) -> str:
        """è·å–å½“å‰æ•°æ®æº"""
        return self.current_source.value
    
    def get_available_sources(self) -> List[str]:
        """è·å–å¯ç”¨æ•°æ®æºåˆ—è¡¨"""
        return [s.value for s in self.available_sources]
```

### 3. æ•°æ®å¤„ç†å±‚ (Data Processing Layer)

#### æ•°æ®éªŒè¯å’Œæ¸…æ´—
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/interface.py`

```python
def validate_and_clean_data(data, data_type: str):
    """æ•°æ®éªŒè¯å’Œæ¸…æ´—
    
    Args:
        data: åŸå§‹æ•°æ®
        data_type: æ•°æ®ç±»å‹
    
    Returns:
        å¤„ç†åçš„æ•°æ®
    """
    if data is None or (hasattr(data, 'empty') and data.empty):
        return None
    
    try:
        if data_type == "stock_data":
            # è‚¡ç¥¨æ•°æ®éªŒè¯
            required_columns = ['open', 'high', 'low', 'close', 'volume']
            if hasattr(data, 'columns'):
                missing_cols = [col for col in required_columns if col not in data.columns]
                if missing_cols:
                    logger.warning(f"âš ï¸ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
                
                # æ•°æ®æ¸…æ´—
                data = data.dropna()  # åˆ é™¤ç©ºå€¼
                data = data[data['volume'] > 0]  # åˆ é™¤æ— äº¤æ˜“é‡çš„æ•°æ®
        
        elif data_type == "news_data":
            # æ–°é—»æ•°æ®éªŒè¯
            if isinstance(data, str) and len(data.strip()) == 0:
                return None
        
        return data
    except Exception as e:
        logger.error(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")
        return None
```

### 4. æ•°æ®å­˜å‚¨å±‚ (Data Storage Layer)

#### ç¼“å­˜ç³»ç»Ÿ
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/config.py`

```python
import os
from typing import Dict, Any

# å…¨å±€é…ç½®
_config = None

def get_config() -> Dict[str, Any]:
    """è·å–æ•°æ®æµé…ç½®"""
    global _config
    if _config is None:
        _config = {
            "data_dir": os.path.join(os.path.expanduser("~"), "Documents", "TradingAgents", "data"),
            "cache_dir": os.path.join(os.path.expanduser("~"), "Documents", "TradingAgents", "cache"),
            "cache_expiry": {
                "market_data": 300,      # 5åˆ†é’Ÿ
                "news_data": 3600,       # 1å°æ—¶
                "fundamentals": 86400,   # 24å°æ—¶
                "social_sentiment": 1800, # 30åˆ†é’Ÿ
            },
            "max_cache_size": 1000,  # æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
            "enable_cache": True,
        }
    return _config

def set_config(config: Dict[str, Any]):
    """è®¾ç½®æ•°æ®æµé…ç½®"""
    global _config
    _config = config

# æ•°æ®ç›®å½•
DATA_DIR = get_config()["data_dir"]
CACHE_DIR = get_config()["cache_dir"]

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(CACHE_DIR, exist_ok=True)
```

### 5. æ•°æ®åˆ†å‘å±‚ (Data Distribution Layer)

#### ç»Ÿä¸€æ•°æ®æ¥å£
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/interface.py`

```python
# ç»Ÿä¸€æ•°æ®è·å–æ¥å£
def get_finnhub_news(
    ticker: Annotated[str, "å…¬å¸è‚¡ç¥¨ä»£ç ï¼Œå¦‚ 'AAPL', 'TSM' ç­‰"],
    curr_date: Annotated[str, "å½“å‰æ—¥æœŸï¼Œæ ¼å¼ä¸º yyyy-mm-dd"],
    look_back_days: Annotated[int, "å›çœ‹å¤©æ•°"],
):
    """è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„å…¬å¸æ–°é—»
    
    Args:
        ticker (str): ç›®æ ‡å…¬å¸çš„è‚¡ç¥¨ä»£ç 
        curr_date (str): å½“å‰æ—¥æœŸï¼Œæ ¼å¼ä¸º yyyy-mm-dd
        look_back_days (int): å›çœ‹å¤©æ•°
    
    Returns:
        str: åŒ…å«å…¬å¸æ–°é—»çš„æ•°æ®æ¡†
    """
    start_date = datetime.strptime(curr_date, "%Y-%m-%d")
    before = start_date - relativedelta(days=look_back_days)
    before = before.strftime("%Y-%m-%d")
    
    result = get_data_in_range(ticker, before, curr_date, "news_data", DATA_DIR)
    
    if len(result) == 0:
        error_msg = f"âš ï¸ æ— æ³•è·å–{ticker}çš„æ–°é—»æ•°æ® ({before} åˆ° {curr_date})\n"
        error_msg += f"å¯èƒ½çš„åŸå› ï¼š\n"
        error_msg += f"1. æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·¯å¾„é…ç½®é”™è¯¯\n"
        error_msg += f"2. æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æ²¡æœ‰æ–°é—»æ•°æ®\n"
        error_msg += f"3. éœ€è¦å…ˆä¸‹è½½æˆ–æ›´æ–°Finnhubæ–°é—»æ•°æ®\n"
        error_msg += f"å»ºè®®ï¼šæ£€æŸ¥æ•°æ®ç›®å½•é…ç½®æˆ–é‡æ–°è·å–æ–°é—»æ•°æ®"
        logger.debug(f"ğŸ“° [DEBUG] {error_msg}")
        return error_msg
    
    combined_result = ""
    for day, data in result.items():
        if len(data) == 0:
            continue
        for entry in data:
            current_news = (
                "### " + entry["headline"] + f" ({day})" + "\n" + entry["summary"]
            )
            combined_result += current_news + "\n\n"
    
    return f"## {ticker} News, from {before} to {curr_date}:\n" + str(combined_result)

def get_finnhub_company_insider_sentiment(
    ticker: Annotated[str, "è‚¡ç¥¨ä»£ç "],
    curr_date: Annotated[str, "å½“å‰äº¤æ˜“æ—¥æœŸï¼Œyyyy-mm-ddæ ¼å¼"],
    look_back_days: Annotated[int, "å›çœ‹å¤©æ•°"],
):
    """è·å–å…¬å¸å†…éƒ¨äººå£«æƒ…ç»ªæ•°æ®ï¼ˆæ¥è‡ªå…¬å¼€SECä¿¡æ¯ï¼‰
    
    Args:
        ticker (str): å…¬å¸è‚¡ç¥¨ä»£ç 
        curr_date (str): å½“å‰äº¤æ˜“æ—¥æœŸï¼Œyyyy-mm-ddæ ¼å¼
        look_back_days (int): å›çœ‹å¤©æ•°
    
    Returns:
        str: è¿‡å»æŒ‡å®šå¤©æ•°çš„æƒ…ç»ªæŠ¥å‘Š
    """
    date_obj = datetime.strptime(curr_date, "%Y-%m-%d")
    before = date_obj - relativedelta(days=look_back_days)
    before = before.strftime("%Y-%m-%d")
    
    data = get_data_in_range(ticker, before, curr_date, "insider_senti", DATA_DIR)
    
    if len(data) == 0:
        return ""
    
    result_str = ""
    seen_dicts = []
    for date, senti_list in data.items():
        for entry in senti_list:
            if entry not in seen_dicts:
                result_str += f"### {entry['year']}-{entry['month']}:\nChange: {entry['change']}\nMonthly Share Purchase Ratio: {entry['mspr']}\n\n"
                seen_dicts.append(entry)
    
    return (
        f"## {ticker} Insider Sentiment Data for {before} to {curr_date}:\n"
        + result_str
        + "The change field refers to the net buying/selling from all insiders' transactions. The mspr field refers to monthly share purchase ratio."
    )
```

### 6. å·¥å…·é›†æˆå±‚ (Tool Integration Layer)

#### Toolkit ç»Ÿä¸€å·¥å…·åŒ…
**æ–‡ä»¶ä½ç½®**: `tradingagents/agents/utils/agent_utils.py`

```python
class Toolkit:
    """ç»Ÿä¸€å·¥å…·åŒ…ï¼Œä¸ºæ‰€æœ‰æ™ºèƒ½ä½“æä¾›æ•°æ®è®¿é—®æ¥å£"""
    
    def __init__(self, config):
        self.config = config
        self.logger = get_logger('agents')
    
    def get_stock_fundamentals_unified(self, ticker: str):
        """ç»Ÿä¸€åŸºæœ¬é¢åˆ†æå·¥å…·ï¼Œè‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹"""
        from tradingagents.utils.stock_utils import StockUtils
        
        try:
            market_info = StockUtils.get_market_info(ticker)
            
            if market_info['market_type'] == 'Aè‚¡':
                return self._get_china_stock_fundamentals(ticker)
            elif market_info['market_type'] == 'æ¸¯è‚¡':
                return self._get_hk_stock_fundamentals(ticker)
            else:
                return self._get_us_stock_fundamentals(ticker)
        except Exception as e:
            self.logger.error(f"åŸºæœ¬é¢æ•°æ®è·å–å¤±è´¥: {e}")
            return f"âŒ åŸºæœ¬é¢æ•°æ®è·å–å¤±è´¥: {str(e)}"
    
    def _get_china_stock_fundamentals(self, ticker: str):
        """è·å–ä¸­å›½è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®"""
        try:
            from tradingagents.dataflows.data_source_manager import DataSourceManager
            
            manager = DataSourceManager()
            current_source = manager.get_current_source()
            
            if current_source == 'tushare':
                return self._get_tushare_fundamentals(ticker)
            elif current_source == 'akshare':
                return self._get_akshare_fundamentals(ticker)
            else:
                # é™çº§ç­–ç•¥
                return self._get_akshare_fundamentals(ticker)
        except Exception as e:
            self.logger.error(f"ä¸­å›½è‚¡ç¥¨åŸºæœ¬é¢è·å–å¤±è´¥: {e}")
            return f"âŒ ä¸­å›½è‚¡ç¥¨åŸºæœ¬é¢è·å–å¤±è´¥: {str(e)}"
    
    def _get_tushare_fundamentals(self, ticker: str):
        """ä½¿ç”¨Tushareè·å–åŸºæœ¬é¢æ•°æ®"""
        try:
            from tradingagents.dataflows.tushare_utils import TushareProvider
            
            provider = TushareProvider()
            
            # è·å–åŸºæœ¬ä¿¡æ¯
            basic_info = provider.get_stock_basic(ticker)
            
            # è·å–è´¢åŠ¡æ•°æ®
            financial_data = provider.get_financial_data(ticker)
            
            # æ ¼å¼åŒ–è¾“å‡º
            report = f"""## {ticker} åŸºæœ¬é¢åˆ†ææŠ¥å‘Š (Tushareæ•°æ®æº)
            
**åŸºæœ¬ä¿¡æ¯**:
- è‚¡ç¥¨åç§°: {basic_info.get('name', 'N/A')}
- æ‰€å±è¡Œä¸š: {basic_info.get('industry', 'N/A')}
- ä¸Šå¸‚æ—¥æœŸ: {basic_info.get('list_date', 'N/A')}

**è´¢åŠ¡æŒ‡æ ‡**:
- æ€»å¸‚å€¼: {financial_data.get('total_mv', 'N/A')}
- å¸‚ç›ˆç‡: {financial_data.get('pe', 'N/A')}
- å¸‚å‡€ç‡: {financial_data.get('pb', 'N/A')}
- å‡€èµ„äº§æ”¶ç›Šç‡: {financial_data.get('roe', 'N/A')}
            """
            
            return report
        except Exception as e:
            self.logger.error(f"TushareåŸºæœ¬é¢è·å–å¤±è´¥: {e}")
            return f"âŒ TushareåŸºæœ¬é¢è·å–å¤±è´¥: {str(e)}"
```

#### è‚¡ç¥¨å·¥å…·
**æ–‡ä»¶ä½ç½®**: `tradingagents/utils/stock_utils.py`

```python
from enum import Enum
from typing import Dict, Any

class StockMarket(Enum):
    """è‚¡ç¥¨å¸‚åœºæšä¸¾"""
    CHINA_A = "china_a"      # ä¸­å›½Aè‚¡
    HONG_KONG = "hong_kong"  # æ¸¯è‚¡
    US = "us"                # ç¾è‚¡
    UNKNOWN = "unknown"      # æœªçŸ¥å¸‚åœº

class StockUtils:
    """è‚¡ç¥¨å·¥å…·ç±»"""
    
    @staticmethod
    def identify_stock_market(ticker: str) -> StockMarket:
        """è¯†åˆ«è‚¡ç¥¨æ‰€å±å¸‚åœº
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            
        Returns:
            StockMarket: è‚¡ç¥¨å¸‚åœºç±»å‹
        """
        ticker = ticker.upper().strip()
        
        # ä¸­å›½Aè‚¡åˆ¤æ–­
        if (ticker.isdigit() and len(ticker) == 6 and 
            (ticker.startswith('0') or ticker.startswith('3') or ticker.startswith('6'))):
            return StockMarket.CHINA_A
        
        # æ¸¯è‚¡åˆ¤æ–­
        if (ticker.isdigit() and len(ticker) <= 5) or ticker.endswith('.HK'):
            return StockMarket.HONG_KONG
        
        # ç¾è‚¡åˆ¤æ–­ï¼ˆå­—æ¯å¼€å¤´æˆ–åŒ…å«å­—æ¯ï¼‰
        if any(c.isalpha() for c in ticker) and not ticker.endswith('.HK'):
            return StockMarket.US
        
        return StockMarket.UNKNOWN
    
    @staticmethod
    def get_market_info(ticker: str) -> Dict[str, Any]:
        """è·å–è‚¡ç¥¨å¸‚åœºä¿¡æ¯
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            
        Returns:
            Dict: å¸‚åœºä¿¡æ¯å­—å…¸
        """
        market = StockUtils.identify_stock_market(ticker)
        
        market_info = {
            StockMarket.CHINA_A: {
                'market_type': 'Aè‚¡',
                'market_name': 'ä¸­å›½Aè‚¡å¸‚åœº',
                'currency_name': 'äººæ°‘å¸',
                'currency_symbol': 'Â¥',
                'timezone': 'Asia/Shanghai',
                'trading_hours': '09:30-15:00'
            },
            StockMarket.HONG_KONG: {
                'market_type': 'æ¸¯è‚¡',
                'market_name': 'é¦™æ¸¯è‚¡ç¥¨å¸‚åœº',
                'currency_name': 'æ¸¯å¸',
                'currency_symbol': 'HK$',
                'timezone': 'Asia/Hong_Kong',
                'trading_hours': '09:30-16:00'
            },
            StockMarket.US: {
                'market_type': 'ç¾è‚¡',
                'market_name': 'ç¾å›½è‚¡ç¥¨å¸‚åœº',
                'currency_name': 'ç¾å…ƒ',
                'currency_symbol': '$',
                'timezone': 'America/New_York',
                'trading_hours': '09:30-16:00'
            },
            StockMarket.UNKNOWN: {
                'market_type': 'æœªçŸ¥',
                'market_name': 'æœªçŸ¥å¸‚åœº',
                'currency_name': 'æœªçŸ¥',
                'currency_symbol': '?',
                'timezone': 'UTC',
                'trading_hours': 'Unknown'
            }
        }
        
        return market_info.get(market, market_info[StockMarket.UNKNOWN])
    
    @staticmethod
    def get_data_source(ticker: str) -> str:
        """æ ¹æ®è‚¡ç¥¨ä»£ç è·å–æ¨èçš„æ•°æ®æº
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            
        Returns:
            str: æ•°æ®æºåç§°
        """
        market = StockUtils.identify_stock_market(ticker)
        
        if market == StockMarket.CHINA_A:
            return "china_unified"  # ä½¿ç”¨ç»Ÿä¸€çš„ä¸­å›½è‚¡ç¥¨æ•°æ®æº
        elif market == StockMarket.HONG_KONG:
            return "yahoo_finance"  # æ¸¯è‚¡ä½¿ç”¨Yahoo Finance
        elif market == StockMarket.US:
            return "yahoo_finance"  # ç¾è‚¡ä½¿ç”¨Yahoo Finance
        else:
            return "unknown"
```

## ğŸ”„ æ•°æ®æµè½¬è¿‡ç¨‹

### å®Œæ•´æ•°æ®æµç¨‹å›¾

```mermaid
sequenceDiagram
    participant Agent as æ™ºèƒ½ä½“
    participant Toolkit as å·¥å…·åŒ…
    participant Interface as æ•°æ®æ¥å£
    participant Manager as æ•°æ®æºç®¡ç†å™¨
    participant Cache as ç¼“å­˜ç³»ç»Ÿ
    participant Source as æ•°æ®æº
    
    Agent->>Toolkit: è¯·æ±‚è‚¡ç¥¨æ•°æ®
    Toolkit->>Interface: è°ƒç”¨ç»Ÿä¸€æ¥å£
    Interface->>Cache: æ£€æŸ¥ç¼“å­˜
    
    alt ç¼“å­˜å‘½ä¸­
        Cache->>Interface: è¿”å›ç¼“å­˜æ•°æ®
    else ç¼“å­˜æœªå‘½ä¸­
        Interface->>Manager: è·å–æ•°æ®æº
        Manager->>Source: è°ƒç”¨æ•°æ®æºAPI
        Source->>Manager: è¿”å›åŸå§‹æ•°æ®
        Manager->>Interface: è¿”å›å¤„ç†åæ•°æ®
        Interface->>Cache: æ›´æ–°ç¼“å­˜
    end
    
    Interface->>Toolkit: è¿”å›æ ¼å¼åŒ–æ•°æ®
    Toolkit->>Agent: è¿”å›åˆ†æå°±ç»ªæ•°æ®
```

### æ•°æ®å¤„ç†æµæ°´çº¿

1. **æ•°æ®è¯·æ±‚**: æ™ºèƒ½ä½“é€šè¿‡Toolkitè¯·æ±‚æ•°æ®
2. **ç¼“å­˜æ£€æŸ¥**: é¦–å…ˆæ£€æŸ¥æœ¬åœ°ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
3. **æ•°æ®æºé€‰æ‹©**: æ ¹æ®è‚¡ç¥¨ç±»å‹é€‰æ‹©æœ€ä½³æ•°æ®æº
4. **æ•°æ®è·å–**: ä»å¤–éƒ¨APIè·å–åŸå§‹æ•°æ®
5. **æ•°æ®éªŒè¯**: éªŒè¯æ•°æ®å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§
6. **æ•°æ®æ¸…æ´—**: æ¸…ç†å¼‚å¸¸å€¼å’Œç¼ºå¤±æ•°æ®
7. **æ•°æ®æ ‡å‡†åŒ–**: ç»Ÿä¸€æ•°æ®æ ¼å¼å’Œå­—æ®µå
8. **æ•°æ®ç¼“å­˜**: å°†å¤„ç†åçš„æ•°æ®å­˜å…¥ç¼“å­˜
9. **æ•°æ®è¿”å›**: è¿”å›æ ¼å¼åŒ–çš„åˆ†æå°±ç»ªæ•°æ®

## ğŸ“Š æ•°æ®è´¨é‡ç›‘æ§

### æ•°æ®è´¨é‡æŒ‡æ ‡

```python
class DataQualityMonitor:
    """æ•°æ®è´¨é‡ç›‘æ§å™¨"""
    
    def __init__(self):
        self.quality_metrics = {
            'completeness': 0.0,    # å®Œæ•´æ€§
            'accuracy': 0.0,        # å‡†ç¡®æ€§
            'timeliness': 0.0,      # åŠæ—¶æ€§
            'consistency': 0.0,     # ä¸€è‡´æ€§
        }
    
    def check_data_quality(self, data, data_type: str):
        """æ£€æŸ¥æ•°æ®è´¨é‡
        
        Args:
            data: å¾…æ£€æŸ¥çš„æ•°æ®
            data_type: æ•°æ®ç±»å‹
        
        Returns:
            Dict: è´¨é‡è¯„åˆ†
        """
        if data is None:
            return {'overall_score': 0.0, 'issues': ['æ•°æ®ä¸ºç©º']}
        
        issues = []
        scores = {}
        
        # å®Œæ•´æ€§æ£€æŸ¥
        completeness = self._check_completeness(data, data_type)
        scores['completeness'] = completeness
        if completeness < 0.8:
            issues.append(f'æ•°æ®å®Œæ•´æ€§ä¸è¶³: {completeness:.1%}')
        
        # å‡†ç¡®æ€§æ£€æŸ¥
        accuracy = self._check_accuracy(data, data_type)
        scores['accuracy'] = accuracy
        if accuracy < 0.9:
            issues.append(f'æ•°æ®å‡†ç¡®æ€§ä¸è¶³: {accuracy:.1%}')
        
        # åŠæ—¶æ€§æ£€æŸ¥
        timeliness = self._check_timeliness(data, data_type)
        scores['timeliness'] = timeliness
        if timeliness < 0.7:
            issues.append(f'æ•°æ®åŠæ—¶æ€§ä¸è¶³: {timeliness:.1%}')
        
        # è®¡ç®—æ€»åˆ†
        overall_score = sum(scores.values()) / len(scores)
        
        return {
            'overall_score': overall_score,
            'detailed_scores': scores,
            'issues': issues
        }
    
    def _check_completeness(self, data, data_type: str) -> float:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        if data_type == "stock_data":
            required_fields = ['open', 'high', 'low', 'close', 'volume']
            if hasattr(data, 'columns'):
                available_fields = len([f for f in required_fields if f in data.columns])
                return available_fields / len(required_fields)
        return 1.0
    
    def _check_accuracy(self, data, data_type: str) -> float:
        """æ£€æŸ¥æ•°æ®å‡†ç¡®æ€§"""
        if data_type == "stock_data" and hasattr(data, 'columns'):
            # æ£€æŸ¥ä»·æ ¼é€»è¾‘æ€§
            if all(col in data.columns for col in ['high', 'low', 'close']):
                valid_rows = (data['high'] >= data['low']).sum()
                total_rows = len(data)
                return valid_rows / total_rows if total_rows > 0 else 0.0
        return 1.0
    
    def _check_timeliness(self, data, data_type: str) -> float:
        """æ£€æŸ¥æ•°æ®åŠæ—¶æ€§"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…åº”æ£€æŸ¥æ•°æ®æ—¶é—´æˆ³
        return 1.0
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### ç¼“å­˜ç­–ç•¥

```python
class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.cache_dir = config.get('cache_dir', './cache')
        self.cache_expiry = config.get('cache_expiry', {})
        self.max_cache_size = config.get('max_cache_size', 1000)
    
    def get_cache_key(self, ticker: str, data_type: str, params: dict = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        
        key_parts = [ticker, data_type]
        if params:
            key_parts.append(str(sorted(params.items())))
        
        key_string = '|'.join(key_parts)
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def is_cache_valid(self, cache_file: str, data_type: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not os.path.exists(cache_file):
            return False
        
        # æ£€æŸ¥ç¼“å­˜æ—¶é—´
        cache_time = os.path.getmtime(cache_file)
        current_time = time.time()
        expiry_seconds = self.cache_expiry.get(data_type, 3600)
        
        return (current_time - cache_time) < expiry_seconds
    
    def get_from_cache(self, cache_key: str, data_type: str):
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
        
        if self.is_cache_valid(cache_file, data_type):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        return None
    
    def save_to_cache(self, cache_key: str, data, data_type: str):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        try:
            os.makedirs(self.cache_dir, exist_ok=True)
            cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
            
            # åºåˆ—åŒ–æ•°æ®
            if hasattr(data, 'to_dict'):
                serializable_data = data.to_dict()
            elif hasattr(data, 'to_json'):
                serializable_data = json.loads(data.to_json())
            else:
                serializable_data = data
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_data, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"æ•°æ®å·²ç¼“å­˜: {cache_key}")
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
```

### å¹¶è¡Œæ•°æ®è·å–

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Callable

class ParallelDataFetcher:
    """å¹¶è¡Œæ•°æ®è·å–å™¨"""
    
    def __init__(self, max_workers: int = 5):
        self.max_workers = max_workers
    
    def fetch_multiple_data(self, tasks: List[dict]) -> dict:
        """å¹¶è¡Œè·å–å¤šä¸ªæ•°æ®æºçš„æ•°æ®
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å« {'name': str, 'func': callable, 'args': tuple, 'kwargs': dict}
        
        Returns:
            dict: ç»“æœå­—å…¸ï¼Œé”®ä¸ºä»»åŠ¡åç§°ï¼Œå€¼ä¸ºç»“æœ
        """
        results = {}
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_name = {}
            for task in tasks:
                future = executor.submit(
                    task['func'], 
                    *task.get('args', ()), 
                    **task.get('kwargs', {})
                )
                future_to_name[future] = task['name']
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_name):
                task_name = future_to_name[future]
                try:
                    result = future.result(timeout=30)  # 30ç§’è¶…æ—¶
                    results[task_name] = result
                    logger.debug(f"âœ… ä»»åŠ¡å®Œæˆ: {task_name}")
                except Exception as e:
                    logger.error(f"âŒ ä»»åŠ¡å¤±è´¥: {task_name}, é”™è¯¯: {e}")
                    results[task_name] = None
        
        return results
```

## ğŸ›¡ï¸ é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥

### æ•°æ®æºé™çº§

```python
class DataSourceFallback:
    """æ•°æ®æºé™çº§å¤„ç†å™¨"""
    
    def __init__(self, manager: DataSourceManager):
        self.manager = manager
        self.fallback_order = {
            'china_stock': ['tushare', 'akshare', 'baostock'],
            'us_stock': ['yahoo_finance', 'finnhub'],
            'hk_stock': ['yahoo_finance', 'akshare']
        }
    
    def get_data_with_fallback(self, ticker: str, data_type: str, 
                              get_data_func: Callable, *args, **kwargs):
        """ä½¿ç”¨é™çº§ç­–ç•¥è·å–æ•°æ®
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            data_type: æ•°æ®ç±»å‹
            get_data_func: æ•°æ®è·å–å‡½æ•°
            *args, **kwargs: å‡½æ•°å‚æ•°
        
        Returns:
            æ•°æ®æˆ–é”™è¯¯ä¿¡æ¯
        """
        from tradingagents.utils.stock_utils import StockUtils
        
        market_info = StockUtils.get_market_info(ticker)
        market_type = market_info['market_type']
        
        # ç¡®å®šé™çº§é¡ºåº
        if market_type == 'Aè‚¡':
            sources = self.fallback_order['china_stock']
        elif market_type == 'ç¾è‚¡':
            sources = self.fallback_order['us_stock']
        elif market_type == 'æ¸¯è‚¡':
            sources = self.fallback_order['hk_stock']
        else:
            sources = ['yahoo_finance']  # é»˜è®¤
        
        last_error = None
        
        for source in sources:
            try:
                # åˆ‡æ¢æ•°æ®æº
                if source in self.manager.get_available_sources():
                    self.manager.switch_source(source)
                    
                    # å°è¯•è·å–æ•°æ®
                    data = get_data_func(*args, **kwargs)
                    
                    if data is not None and not (hasattr(data, 'empty') and data.empty):
                        logger.info(f"âœ… ä½¿ç”¨{source}æ•°æ®æºæˆåŠŸè·å–{ticker}çš„{data_type}æ•°æ®")
                        return data
                    else:
                        logger.warning(f"âš ï¸ {source}æ•°æ®æºè¿”å›ç©ºæ•°æ®")
                        
            except Exception as e:
                last_error = e
                logger.warning(f"âš ï¸ {source}æ•°æ®æºå¤±è´¥: {e}")
                continue
        
        # æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥
        error_msg = f"âŒ æ‰€æœ‰æ•°æ®æºéƒ½æ— æ³•è·å–{ticker}çš„{data_type}æ•°æ®"
        if last_error:
            error_msg += f"ï¼Œæœ€åé”™è¯¯: {last_error}"
        
        logger.error(error_msg)
        return error_msg
```

## ğŸ“ˆ ç›‘æ§å’Œè§‚æµ‹

### æ•°æ®æµç›‘æ§

```python
class DataFlowMonitor:
    """æ•°æ®æµç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'average_response_time': 0.0,
            'data_source_usage': {},
        }
    
    def record_request(self, ticker: str, data_type: str, 
                      success: bool, response_time: float, 
                      data_source: str, from_cache: bool):
        """è®°å½•æ•°æ®è¯·æ±‚"""
        self.metrics['total_requests'] += 1
        
        if success:
            self.metrics['successful_requests'] += 1
        else:
            self.metrics['failed_requests'] += 1
        
        if from_cache:
            self.metrics['cache_hits'] += 1
        else:
            self.metrics['cache_misses'] += 1
        
        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        total_time = self.metrics['average_response_time'] * (self.metrics['total_requests'] - 1)
        self.metrics['average_response_time'] = (total_time + response_time) / self.metrics['total_requests']
        
        # è®°å½•æ•°æ®æºä½¿ç”¨æƒ…å†µ
        if data_source not in self.metrics['data_source_usage']:
            self.metrics['data_source_usage'][data_source] = 0
        self.metrics['data_source_usage'][data_source] += 1
        
        logger.info(f"ğŸ“Š æ•°æ®è¯·æ±‚è®°å½•: {ticker} {data_type} {'âœ…' if success else 'âŒ'} {response_time:.2f}s {data_source} {'(ç¼“å­˜)' if from_cache else ''}")
    
    def get_metrics_report(self) -> str:
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        if self.metrics['total_requests'] == 0:
            return "ğŸ“Š æš‚æ— æ•°æ®è¯·æ±‚è®°å½•"
        
        success_rate = self.metrics['successful_requests'] / self.metrics['total_requests']
        cache_hit_rate = self.metrics['cache_hits'] / self.metrics['total_requests']
        
        report = f"""ğŸ“Š æ•°æ®æµç›‘æ§æŠ¥å‘Š
        
**è¯·æ±‚ç»Ÿè®¡**:
- æ€»è¯·æ±‚æ•°: {self.metrics['total_requests']}
- æˆåŠŸè¯·æ±‚: {self.metrics['successful_requests']}
- å¤±è´¥è¯·æ±‚: {self.metrics['failed_requests']}
- æˆåŠŸç‡: {success_rate:.1%}

**ç¼“å­˜ç»Ÿè®¡**:
- ç¼“å­˜å‘½ä¸­: {self.metrics['cache_hits']}
- ç¼“å­˜æœªå‘½ä¸­: {self.metrics['cache_misses']}
- ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1%}

**æ€§èƒ½ç»Ÿè®¡**:
- å¹³å‡å“åº”æ—¶é—´: {self.metrics['average_response_time']:.2f}s

**æ•°æ®æºä½¿ç”¨æƒ…å†µ**:
"""
        
        for source, count in self.metrics['data_source_usage'].items():
            usage_rate = count / self.metrics['total_requests']
            report += f"- {source}: {count}æ¬¡ ({usage_rate:.1%})\n"
        
        return report

# å…¨å±€ç›‘æ§å®ä¾‹
data_flow_monitor = DataFlowMonitor()
```

## ğŸ”§ é…ç½®ç®¡ç†

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# .env æ–‡ä»¶ç¤ºä¾‹

# æ•°æ®æºé…ç½®
DEFAULT_CHINA_DATA_SOURCE=tushare
TUSHARE_TOKEN=your_tushare_token_here
FINNHUB_API_KEY=your_finnhub_api_key
REDDIT_CLIENT_ID=your_reddit_client_id
REDDIT_CLIENT_SECRET=your_reddit_client_secret

# æ•°æ®ç›®å½•é…ç½®
DATA_DIR=./data
CACHE_DIR=./cache
RESULTS_DIR=./results

# ç¼“å­˜é…ç½®
ENABLE_CACHE=true
CACHE_EXPIRY_MARKET_DATA=300
CACHE_EXPIRY_NEWS_DATA=3600
CACHE_EXPIRY_FUNDAMENTALS=86400
MAX_CACHE_SIZE=1000

# æ€§èƒ½é…ç½®
MAX_PARALLEL_WORKERS=5
REQUEST_TIMEOUT=30
RETRY_ATTEMPTS=3
RETRY_DELAY=1

# ç›‘æ§é…ç½®
ENABLE_MONITORING=true
LOG_LEVEL=INFO
```

### åŠ¨æ€é…ç½®æ›´æ–°

```python
class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_file: str = None):
        self.config_file = config_file or '.env'
        self.config = self._load_config()
        self._setup_directories()
    
    def _load_config(self) -> dict:
        """åŠ è½½é…ç½®"""
        from dotenv import load_dotenv
        
        load_dotenv(self.config_file)
        
        return {
            # æ•°æ®æºé…ç½®
            'default_china_data_source': os.getenv('DEFAULT_CHINA_DATA_SOURCE', 'tushare'),
            'tushare_token': os.getenv('TUSHARE_TOKEN'),
            'finnhub_api_key': os.getenv('FINNHUB_API_KEY'),
            'reddit_client_id': os.getenv('REDDIT_CLIENT_ID'),
            'reddit_client_secret': os.getenv('REDDIT_CLIENT_SECRET'),
            
            # ç›®å½•é…ç½®
            'data_dir': os.getenv('DATA_DIR', './data'),
            'cache_dir': os.getenv('CACHE_DIR', './cache'),
            'results_dir': os.getenv('RESULTS_DIR', './results'),
            
            # ç¼“å­˜é…ç½®
            'enable_cache': os.getenv('ENABLE_CACHE', 'true').lower() == 'true',
            'cache_expiry': {
                'market_data': int(os.getenv('CACHE_EXPIRY_MARKET_DATA', '300')),
                'news_data': int(os.getenv('CACHE_EXPIRY_NEWS_DATA', '3600')),
                'fundamentals': int(os.getenv('CACHE_EXPIRY_FUNDAMENTALS', '86400')),
            },
            'max_cache_size': int(os.getenv('MAX_CACHE_SIZE', '1000')),
            
            # æ€§èƒ½é…ç½®
            'max_parallel_workers': int(os.getenv('MAX_PARALLEL_WORKERS', '5')),
            'request_timeout': int(os.getenv('REQUEST_TIMEOUT', '30')),
            'retry_attempts': int(os.getenv('RETRY_ATTEMPTS', '3')),
            'retry_delay': float(os.getenv('RETRY_DELAY', '1.0')),
            
            # ç›‘æ§é…ç½®
            'enable_monitoring': os.getenv('ENABLE_MONITORING', 'true').lower() == 'true',
            'log_level': os.getenv('LOG_LEVEL', 'INFO'),
        }
    
    def _setup_directories(self):
        """è®¾ç½®ç›®å½•"""
        for dir_key in ['data_dir', 'cache_dir', 'results_dir']:
            dir_path = self.config[dir_key]
            os.makedirs(dir_path, exist_ok=True)
            logger.info(f"ğŸ“ ç›®å½•å·²å‡†å¤‡: {dir_key} = {dir_path}")
    
    def get(self, key: str, default=None):
        """è·å–é…ç½®å€¼"""
        return self.config.get(key, default)
    
    def update(self, key: str, value):
        """æ›´æ–°é…ç½®å€¼"""
        self.config[key] = value
        logger.info(f"ğŸ”§ é…ç½®å·²æ›´æ–°: {key} = {value}")
    
    def reload(self):
        """é‡æ–°åŠ è½½é…ç½®"""
        self.config = self._load_config()
        self._setup_directories()
        logger.info("ğŸ”„ é…ç½®å·²é‡æ–°åŠ è½½")

# å…¨å±€é…ç½®å®ä¾‹
config_manager = ConfigManager()
```

## ğŸš€ æœ€ä½³å®è·µ

### 1. æ•°æ®æºé€‰æ‹©ç­–ç•¥

```python
# æ¨èçš„æ•°æ®æºé…ç½®
RECOMMENDED_DATA_SOURCES = {
    'Aè‚¡': {
        'primary': 'tushare',      # ä¸»è¦æ•°æ®æºï¼šä¸“ä¸šã€ç¨³å®š
        'fallback': ['akshare', 'baostock'],  # å¤‡ç”¨æ•°æ®æº
        'use_case': 'é€‚ç”¨äºä¸“ä¸šæŠ•èµ„åˆ†æï¼Œæ•°æ®è´¨é‡é«˜'
    },
    'æ¸¯è‚¡': {
        'primary': 'yahoo_finance',
        'fallback': ['akshare'],
        'use_case': 'å›½é™…åŒ–æ•°æ®æºï¼Œè¦†ç›–å…¨é¢'
    },
    'ç¾è‚¡': {
        'primary': 'yahoo_finance',
        'fallback': ['finnhub'],
        'use_case': 'å…è´¹ä¸”ç¨³å®šçš„ç¾è‚¡æ•°æ®'
    }
}
```

### 2. ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

```python
# ç¼“å­˜è¿‡æœŸæ—¶é—´å»ºè®®
CACHE_EXPIRY_RECOMMENDATIONS = {
    'real_time_data': 60,        # å®æ—¶æ•°æ®ï¼š1åˆ†é’Ÿ
    'intraday_data': 300,        # æ—¥å†…æ•°æ®ï¼š5åˆ†é’Ÿ
    'daily_data': 3600,          # æ—¥çº¿æ•°æ®ï¼š1å°æ—¶
    'fundamental_data': 86400,   # åŸºæœ¬é¢æ•°æ®ï¼š24å°æ—¶
    'news_data': 1800,           # æ–°é—»æ•°æ®ï¼š30åˆ†é’Ÿ
    'social_sentiment': 900,     # ç¤¾äº¤æƒ…ç»ªï¼š15åˆ†é’Ÿ
}
```

### 3. é”™è¯¯å¤„ç†æ¨¡å¼

```python
# é”™è¯¯å¤„ç†æœ€ä½³å®è·µ
def robust_data_fetch(func):
    """æ•°æ®è·å–è£…é¥°å™¨ï¼Œæä¾›ç»Ÿä¸€çš„é”™è¯¯å¤„ç†"""
    def wrapper(*args, **kwargs):
        max_retries = 3
        retry_delay = 1.0
        
        for attempt in range(max_retries):
            try:
                result = func(*args, **kwargs)
                if result is not None:
                    return result
                else:
                    logger.warning(f"ç¬¬{attempt + 1}æ¬¡å°è¯•è¿”å›ç©ºæ•°æ®")
            except Exception as e:
                logger.warning(f"ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay * (2 ** attempt))  # æŒ‡æ•°é€€é¿
                else:
                    logger.error(f"æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œæœ€ç»ˆé”™è¯¯: {e}")
                    return None
        
        return None
    return wrapper
```

### 4. æ€§èƒ½ç›‘æ§å»ºè®®

```python
# æ€§èƒ½ç›‘æ§å…³é”®æŒ‡æ ‡
PERFORMANCE_THRESHOLDS = {
    'response_time': {
        'excellent': 1.0,    # 1ç§’ä»¥å†…
        'good': 3.0,         # 3ç§’ä»¥å†…
        'acceptable': 10.0,  # 10ç§’ä»¥å†…
    },
    'success_rate': {
        'excellent': 0.99,   # 99%ä»¥ä¸Š
        'good': 0.95,        # 95%ä»¥ä¸Š
        'acceptable': 0.90,  # 90%ä»¥ä¸Š
    },
    'cache_hit_rate': {
        'excellent': 0.80,   # 80%ä»¥ä¸Š
        'good': 0.60,        # 60%ä»¥ä¸Š
        'acceptable': 0.40,  # 40%ä»¥ä¸Š
    }
}
```

## ğŸ“‹ æ€»ç»“

TradingAgents çš„æ•°æ®æµæ¶æ„å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

### âœ… ä¼˜åŠ¿

1. **ç»Ÿä¸€æ¥å£**: é€šè¿‡ç»Ÿä¸€çš„æ•°æ®æ¥å£å±è”½åº•å±‚æ•°æ®æºå·®å¼‚
2. **æ™ºèƒ½é™çº§**: è‡ªåŠ¨æ•°æ®æºåˆ‡æ¢ï¼Œç¡®ä¿æ•°æ®è·å–çš„å¯é æ€§
3. **é«˜æ•ˆç¼“å­˜**: å¤šå±‚ç¼“å­˜ç­–ç•¥ï¼Œæ˜¾è‘—æå‡å“åº”é€Ÿåº¦
4. **è´¨é‡ç›‘æ§**: å®æ—¶æ•°æ®è´¨é‡æ£€æŸ¥å’Œæ€§èƒ½ç›‘æ§
5. **çµæ´»æ‰©å±•**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ·»åŠ æ–°çš„æ•°æ®æº
6. **é”™è¯¯æ¢å¤**: å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

### ğŸ¯ é€‚ç”¨åœºæ™¯

- **å¤šå¸‚åœºäº¤æ˜“**: æ”¯æŒAè‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡çš„ç»Ÿä¸€æ•°æ®è®¿é—®
- **å®æ—¶åˆ†æ**: ä½å»¶è¿Ÿçš„æ•°æ®è·å–å’Œå¤„ç†
- **å¤§è§„æ¨¡éƒ¨ç½²**: æ”¯æŒé«˜å¹¶å‘å’Œå¤§æ•°æ®é‡å¤„ç†
- **ç ”ç©¶å¼€å‘**: çµæ´»çš„æ•°æ®æºé…ç½®å’Œæ‰©å±•èƒ½åŠ›

### ğŸ”® æœªæ¥å‘å±•

1. **å®æ—¶æ•°æ®æµ**: é›†æˆWebSocketå®æ—¶æ•°æ®æ¨é€
2. **æœºå™¨å­¦ä¹ **: æ•°æ®è´¨é‡æ™ºèƒ½è¯„ä¼°å’Œé¢„æµ‹
3. **äº‘åŸç”Ÿ**: æ”¯æŒäº‘ç«¯æ•°æ®æºå’Œåˆ†å¸ƒå¼ç¼“å­˜
4. **å›½é™…åŒ–**: æ‰©å±•æ›´å¤šå›½é™…å¸‚åœºæ•°æ®æº

é€šè¿‡è¿™ä¸ªæ•°æ®æµæ¶æ„ï¼ŒTradingAgents èƒ½å¤Ÿä¸ºæ™ºèƒ½ä½“æä¾›é«˜è´¨é‡ã€é«˜å¯ç”¨çš„é‡‘èæ•°æ®æœåŠ¡ï¼Œæ”¯æ’‘å¤æ‚çš„æŠ•èµ„å†³ç­–åˆ†æã€‚