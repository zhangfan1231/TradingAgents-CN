#!/usr/bin/env python3
"""
è‚¡ç¥¨æ•°æ®ç¼“å­˜ç®¡ç†å™¨
æ”¯æŒæœ¬åœ°ç¼“å­˜è‚¡ç¥¨æ•°æ®ï¼Œå‡å°‘APIè°ƒç”¨ï¼Œæé«˜å“åº”é€Ÿåº¦
"""

import os
import json
import pickle
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, Any, Union, List
import hashlib

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('agents')


class StockDataCache:
    """è‚¡ç¥¨æ•°æ®ç¼“å­˜ç®¡ç†å™¨ - æ”¯æŒç¾è‚¡å’ŒAè‚¡æ•°æ®ç¼“å­˜ä¼˜åŒ–"""

    def __init__(self, cache_dir: str = None):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨

        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸º tradingagents/dataflows/data_cache
        """
        if cache_dir is None:
            # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
            current_dir = Path(__file__).parent
            cache_dir = current_dir / "data_cache"

        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

        # åˆ›å»ºå­ç›®å½• - æŒ‰å¸‚åœºåˆ†ç±»
        self.us_stock_dir = self.cache_dir / "us_stocks"
        self.china_stock_dir = self.cache_dir / "china_stocks"
        self.us_news_dir = self.cache_dir / "us_news"
        self.china_news_dir = self.cache_dir / "china_news"
        self.us_fundamentals_dir = self.cache_dir / "us_fundamentals"
        self.china_fundamentals_dir = self.cache_dir / "china_fundamentals"
        self.metadata_dir = self.cache_dir / "metadata"

        # åˆ›å»ºæ‰€æœ‰ç›®å½•
        for dir_path in [self.us_stock_dir, self.china_stock_dir, self.us_news_dir,
                        self.china_news_dir, self.us_fundamentals_dir,
                        self.china_fundamentals_dir, self.metadata_dir]:
            dir_path.mkdir(exist_ok=True)

        # ç¼“å­˜é…ç½® - é’ˆå¯¹ä¸åŒå¸‚åœºè®¾ç½®ä¸åŒçš„TTL
        self.cache_config = {
            'us_stock_data': {
                'ttl_hours': 2,  # ç¾è‚¡æ•°æ®ç¼“å­˜2å°æ—¶ï¼ˆè€ƒè™‘åˆ°APIé™åˆ¶ï¼‰
                'max_files': 1000,
                'description': 'ç¾è‚¡å†å²æ•°æ®'
            },
            'china_stock_data': {
                'ttl_hours': 1,  # Aè‚¡æ•°æ®ç¼“å­˜1å°æ—¶ï¼ˆå®æ—¶æ€§è¦æ±‚é«˜ï¼‰
                'max_files': 1000,
                'description': 'Aè‚¡å†å²æ•°æ®'
            },
            'us_news': {
                'ttl_hours': 6,  # ç¾è‚¡æ–°é—»ç¼“å­˜6å°æ—¶
                'max_files': 500,
                'description': 'ç¾è‚¡æ–°é—»æ•°æ®'
            },
            'china_news': {
                'ttl_hours': 4,  # Aè‚¡æ–°é—»ç¼“å­˜4å°æ—¶
                'max_files': 500,
                'description': 'Aè‚¡æ–°é—»æ•°æ®'
            },
            'us_fundamentals': {
                'ttl_hours': 24,  # ç¾è‚¡åŸºæœ¬é¢æ•°æ®ç¼“å­˜24å°æ—¶
                'max_files': 200,
                'description': 'ç¾è‚¡åŸºæœ¬é¢æ•°æ®'
            },
            'china_fundamentals': {
                'ttl_hours': 12,  # Aè‚¡åŸºæœ¬é¢æ•°æ®ç¼“å­˜12å°æ—¶
                'max_files': 200,
                'description': 'Aè‚¡åŸºæœ¬é¢æ•°æ®'
            }
        }

        # å†…å®¹é•¿åº¦é™åˆ¶é…ç½®ï¼ˆæ–‡ä»¶ç¼“å­˜é»˜è®¤ä¸é™åˆ¶ï¼‰
        self.content_length_config = {
            'max_content_length': int(os.getenv('MAX_CACHE_CONTENT_LENGTH', '50000')),  # 50Kå­—ç¬¦
            'long_text_providers': ['dashscope', 'openai', 'google'],  # æ”¯æŒé•¿æ–‡æœ¬çš„æä¾›å•†
            'enable_length_check': os.getenv('ENABLE_CACHE_LENGTH_CHECK', 'false').lower() == 'true'  # æ–‡ä»¶ç¼“å­˜é»˜è®¤ä¸é™åˆ¶
        }

        logger.info(f"ğŸ“ ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜ç›®å½•: {self.cache_dir}")
        logger.info(f"ğŸ—„ï¸ æ•°æ®åº“ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ç¾è‚¡æ•°æ®: âœ… å·²é…ç½®")
        logger.info(f"   Aè‚¡æ•°æ®: âœ… å·²é…ç½®")

    def _determine_market_type(self, symbol: str) -> str:
        """æ ¹æ®è‚¡ç¥¨ä»£ç ç¡®å®šå¸‚åœºç±»å‹"""
        import re

        # åˆ¤æ–­æ˜¯å¦ä¸ºä¸­å›½Aè‚¡ï¼ˆ6ä½æ•°å­—ï¼‰
        if re.match(r'^\d{6}$', str(symbol)):
            return 'china'
        else:
            return 'us'

    def _check_provider_availability(self) -> List[str]:
        """æ£€æŸ¥å¯ç”¨çš„LLMæä¾›å•†"""
        available_providers = []
        
        # æ£€æŸ¥DashScope
        dashscope_key = os.getenv("DASHSCOPE_API_KEY")
        if dashscope_key and dashscope_key.strip():
            available_providers.append('dashscope')
        
        # æ£€æŸ¥OpenAI
        openai_key = os.getenv("OPENAI_API_KEY")
        if openai_key and openai_key.strip():
            # ç®€å•çš„æ ¼å¼æ£€æŸ¥
            if openai_key.startswith('sk-') and len(openai_key) >= 40:
                available_providers.append('openai')
        
        # æ£€æŸ¥Google AI
        google_key = os.getenv("GOOGLE_API_KEY")
        if google_key and google_key.strip():
            available_providers.append('google')
        
        # æ£€æŸ¥Anthropic
        anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        if anthropic_key and anthropic_key.strip():
            available_providers.append('anthropic')
        
        return available_providers

    def should_skip_cache_for_content(self, content: str, data_type: str = "unknown") -> bool:
        """
        åˆ¤æ–­æ˜¯å¦å› ä¸ºå†…å®¹è¶…é•¿è€Œè·³è¿‡ç¼“å­˜
        
        Args:
            content: è¦ç¼“å­˜çš„å†…å®¹
            data_type: æ•°æ®ç±»å‹ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        
        Returns:
            bool: æ˜¯å¦åº”è¯¥è·³è¿‡ç¼“å­˜
        """
        # å¦‚æœæœªå¯ç”¨é•¿åº¦æ£€æŸ¥ï¼Œç›´æ¥è¿”å›False
        if not self.content_length_config['enable_length_check']:
            return False
        
        # æ£€æŸ¥å†…å®¹é•¿åº¦
        content_length = len(content)
        max_length = self.content_length_config['max_content_length']
        
        if content_length <= max_length:
            return False
        
        # å†…å®¹è¶…é•¿ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„é•¿æ–‡æœ¬å¤„ç†æä¾›å•†
        available_providers = self._check_provider_availability()
        long_text_providers = self.content_length_config['long_text_providers']
        
        # æ‰¾åˆ°å¯ç”¨çš„é•¿æ–‡æœ¬æä¾›å•†
        available_long_providers = [p for p in available_providers if p in long_text_providers]
        
        if not available_long_providers:
            logger.warning(f"âš ï¸ å†…å®¹è¿‡é•¿({content_length:,}å­—ç¬¦ > {max_length:,}å­—ç¬¦)ä¸”æ— å¯ç”¨é•¿æ–‡æœ¬æä¾›å•†ï¼Œè·³è¿‡{data_type}ç¼“å­˜")
            logger.info(f"ğŸ’¡ å¯ç”¨æä¾›å•†: {available_providers}")
            logger.info(f"ğŸ’¡ é•¿æ–‡æœ¬æä¾›å•†: {long_text_providers}")
            return True
        else:
            logger.info(f"âœ… å†…å®¹è¾ƒé•¿({content_length:,}å­—ç¬¦)ä½†æœ‰å¯ç”¨é•¿æ–‡æœ¬æä¾›å•†({available_long_providers})ï¼Œç»§ç»­ç¼“å­˜")
            return False
    
    def _generate_cache_key(self, data_type: str, symbol: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å‚æ•°çš„å­—ç¬¦ä¸²
        params_str = f"{data_type}_{symbol}"
        for key, value in sorted(kwargs.items()):
            params_str += f"_{key}_{value}"
        
        # ä½¿ç”¨MD5ç”ŸæˆçŸ­çš„å”¯ä¸€æ ‡è¯†
        cache_key = hashlib.md5(params_str.encode()).hexdigest()[:12]
        return f"{symbol}_{data_type}_{cache_key}"
    
    def _get_cache_path(self, data_type: str, cache_key: str, file_format: str = "json", symbol: str = None) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„ - æ”¯æŒå¸‚åœºåˆ†ç±»"""
        if symbol:
            market_type = self._determine_market_type(symbol)
        else:
            # ä»ç¼“å­˜é”®ä¸­å°è¯•æå–å¸‚åœºç±»å‹
            market_type = 'us' if not cache_key.startswith(('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')) else 'china'

        # æ ¹æ®æ•°æ®ç±»å‹å’Œå¸‚åœºç±»å‹é€‰æ‹©ç›®å½•
        if data_type == "stock_data":
            base_dir = self.china_stock_dir if market_type == 'china' else self.us_stock_dir
        elif data_type == "news":
            base_dir = self.china_news_dir if market_type == 'china' else self.us_news_dir
        elif data_type == "fundamentals":
            base_dir = self.china_fundamentals_dir if market_type == 'china' else self.us_fundamentals_dir
        else:
            base_dir = self.cache_dir

        return base_dir / f"{cache_key}.{file_format}"
    
    def _get_metadata_path(self, cache_key: str) -> Path:
        """è·å–å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„"""
        return self.metadata_dir / f"{cache_key}_meta.json"
    
    def _save_metadata(self, cache_key: str, metadata: Dict[str, Any]):
        """ä¿å­˜å…ƒæ•°æ®"""
        metadata_path = self._get_metadata_path(cache_key)
        metadata_path.parent.mkdir(parents=True, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        metadata['cached_at'] = datetime.now().isoformat()
        
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    def _load_metadata(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """åŠ è½½å…ƒæ•°æ®"""
        metadata_path = self._get_metadata_path(cache_key)
        if not metadata_path.exists():
            return None
        
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"âš ï¸ åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
            return None
    
    def is_cache_valid(self, cache_key: str, max_age_hours: int = None, symbol: str = None, data_type: str = None) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ - æ”¯æŒæ™ºèƒ½TTLé…ç½®"""
        metadata = self._load_metadata(cache_key)
        if not metadata:
            return False

        # å¦‚æœæ²¡æœ‰æŒ‡å®šTTLï¼Œæ ¹æ®æ•°æ®ç±»å‹å’Œå¸‚åœºè‡ªåŠ¨ç¡®å®š
        if max_age_hours is None:
            if symbol and data_type:
                market_type = self._determine_market_type(symbol)
                cache_type = f"{market_type}_{data_type}"
                max_age_hours = self.cache_config.get(cache_type, {}).get('ttl_hours', 24)
            else:
                # ä»å…ƒæ•°æ®ä¸­è·å–ä¿¡æ¯
                symbol = metadata.get('symbol', '')
                data_type = metadata.get('data_type', 'stock_data')
                market_type = self._determine_market_type(symbol)
                cache_type = f"{market_type}_{data_type}"
                max_age_hours = self.cache_config.get(cache_type, {}).get('ttl_hours', 24)

        cached_at = datetime.fromisoformat(metadata['cached_at'])
        age = datetime.now() - cached_at

        is_valid = age.total_seconds() < max_age_hours * 3600

        if is_valid:
            market_type = self._determine_market_type(metadata.get('symbol', ''))
            cache_type = f"{market_type}_{metadata.get('data_type', 'stock_data')}"
            desc = self.cache_config.get(cache_type, {}).get('description', 'æ•°æ®')
            logger.info(f"âœ… ç¼“å­˜æœ‰æ•ˆ: {desc} - {metadata.get('symbol')} (å‰©ä½™ {max_age_hours - age.total_seconds()/3600:.1f}h)")

        return is_valid
    
    def save_stock_data(self, symbol: str, data: Union[pd.DataFrame, str],
                       start_date: str = None, end_date: str = None,
                       data_source: str = "unknown") -> str:
        """
        ä¿å­˜è‚¡ç¥¨æ•°æ®åˆ°ç¼“å­˜ - æ”¯æŒç¾è‚¡å’ŒAè‚¡åˆ†ç±»å­˜å‚¨

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data: è‚¡ç¥¨æ•°æ®ï¼ˆDataFrameæˆ–å­—ç¬¦ä¸²ï¼‰
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            data_source: æ•°æ®æºï¼ˆå¦‚ "tdx", "yfinance", "finnhub"ï¼‰

        Returns:
            cache_key: ç¼“å­˜é”®
        """
        # æ£€æŸ¥å†…å®¹é•¿åº¦æ˜¯å¦éœ€è¦è·³è¿‡ç¼“å­˜
        content_to_check = str(data)
        if self.should_skip_cache_for_content(content_to_check, "è‚¡ç¥¨æ•°æ®"):
            # ç”Ÿæˆä¸€ä¸ªè™šæ‹Ÿçš„ç¼“å­˜é”®ï¼Œä½†ä¸å®é™…ä¿å­˜
            market_type = self._determine_market_type(symbol)
            cache_key = self._generate_cache_key("stock_data", symbol,
                                               start_date=start_date,
                                               end_date=end_date,
                                               source=data_source,
                                               market=market_type,
                                               skipped=True)
            logger.info(f"ğŸš« è‚¡ç¥¨æ•°æ®å› å†…å®¹è¿‡é•¿è¢«è·³è¿‡ç¼“å­˜: {symbol} -> {cache_key}")
            return cache_key

        market_type = self._determine_market_type(symbol)
        cache_key = self._generate_cache_key("stock_data", symbol,
                                           start_date=start_date,
                                           end_date=end_date,
                                           source=data_source,
                                           market=market_type)

        # ä¿å­˜æ•°æ®
        if isinstance(data, pd.DataFrame):
            cache_path = self._get_cache_path("stock_data", cache_key, "csv", symbol)
            cache_path.parent.mkdir(parents=True, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
            data.to_csv(cache_path, index=True)
        else:
            cache_path = self._get_cache_path("stock_data", cache_key, "txt", symbol)
            cache_path.parent.mkdir(parents=True, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
            with open(cache_path, 'w', encoding='utf-8') as f:
                f.write(str(data))

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'symbol': symbol,
            'data_type': 'stock_data',
            'market_type': market_type,
            'start_date': start_date,
            'end_date': end_date,
            'data_source': data_source,
            'file_path': str(cache_path),
            'file_format': 'csv' if isinstance(data, pd.DataFrame) else 'txt',
            'content_length': len(content_to_check)
        }
        self._save_metadata(cache_key, metadata)

        # è·å–æè¿°ä¿¡æ¯
        cache_type = f"{market_type}_stock_data"
        desc = self.cache_config.get(cache_type, {}).get('description', 'è‚¡ç¥¨æ•°æ®')
        logger.info(f"ğŸ’¾ {desc}å·²ç¼“å­˜: {symbol} ({data_source}) -> {cache_key}")
        return cache_key
    
    def load_stock_data(self, cache_key: str) -> Optional[Union[pd.DataFrame, str]]:
        """ä»ç¼“å­˜åŠ è½½è‚¡ç¥¨æ•°æ®"""
        metadata = self._load_metadata(cache_key)
        if not metadata:
            return None
        
        cache_path = Path(metadata['file_path'])
        if not cache_path.exists():
            return None
        
        try:
            if metadata['file_format'] == 'csv':
                return pd.read_csv(cache_path, index_col=0)
            else:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    return f.read()
        except Exception as e:
            logger.error(f"âš ï¸ åŠ è½½ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
            return None
    
    def find_cached_stock_data(self, symbol: str, start_date: str = None,
                              end_date: str = None, data_source: str = None,
                              max_age_hours: int = None) -> Optional[str]:
        """
        æŸ¥æ‰¾åŒ¹é…çš„ç¼“å­˜æ•°æ® - æ”¯æŒæ™ºèƒ½å¸‚åœºåˆ†ç±»æŸ¥æ‰¾

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            data_source: æ•°æ®æº
            max_age_hours: æœ€å¤§ç¼“å­˜æ—¶é—´ï¼ˆå°æ—¶ï¼‰ï¼ŒNoneæ—¶ä½¿ç”¨æ™ºèƒ½é…ç½®

        Returns:
            cache_key: å¦‚æœæ‰¾åˆ°æœ‰æ•ˆç¼“å­˜åˆ™è¿”å›ç¼“å­˜é”®ï¼Œå¦åˆ™è¿”å›None
        """
        market_type = self._determine_market_type(symbol)

        # å¦‚æœæ²¡æœ‰æŒ‡å®šTTLï¼Œä½¿ç”¨æ™ºèƒ½é…ç½®
        if max_age_hours is None:
            cache_type = f"{market_type}_stock_data"
            max_age_hours = self.cache_config.get(cache_type, {}).get('ttl_hours', 24)

        # ç”ŸæˆæŸ¥æ‰¾é”®
        search_key = self._generate_cache_key("stock_data", symbol,
                                            start_date=start_date,
                                            end_date=end_date,
                                            source=data_source,
                                            market=market_type)

        # æ£€æŸ¥ç²¾ç¡®åŒ¹é…
        if self.is_cache_valid(search_key, max_age_hours, symbol, 'stock_data'):
            desc = self.cache_config.get(f"{market_type}_stock_data", {}).get('description', 'æ•°æ®')
            logger.info(f"ğŸ¯ æ‰¾åˆ°ç²¾ç¡®åŒ¹é…çš„{desc}: {symbol} -> {search_key}")
            return search_key

        # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼ŒæŸ¥æ‰¾éƒ¨åˆ†åŒ¹é…ï¼ˆç›¸åŒè‚¡ç¥¨ä»£ç çš„å…¶ä»–ç¼“å­˜ï¼‰
        for metadata_file in self.metadata_dir.glob(f"*_meta.json"):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)

                if (metadata.get('symbol') == symbol and
                    metadata.get('data_type') == 'stock_data' and
                    metadata.get('market_type') == market_type and
                    (data_source is None or metadata.get('data_source') == data_source)):

                    cache_key = metadata_file.stem.replace('_meta', '')
                    if self.is_cache_valid(cache_key, max_age_hours, symbol, 'stock_data'):
                        desc = self.cache_config.get(f"{market_type}_stock_data", {}).get('description', 'æ•°æ®')
                        logger.info(f"ğŸ“‹ æ‰¾åˆ°éƒ¨åˆ†åŒ¹é…çš„{desc}: {symbol} -> {cache_key}")
                        return cache_key
            except Exception:
                continue

        desc = self.cache_config.get(f"{market_type}_stock_data", {}).get('description', 'æ•°æ®')
        logger.error(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„{desc}ç¼“å­˜: {symbol}")
        return None
    
    def save_news_data(self, symbol: str, news_data: str, 
                      start_date: str = None, end_date: str = None,
                      data_source: str = "unknown") -> str:
        """ä¿å­˜æ–°é—»æ•°æ®åˆ°ç¼“å­˜"""
        # æ£€æŸ¥å†…å®¹é•¿åº¦æ˜¯å¦éœ€è¦è·³è¿‡ç¼“å­˜
        if self.should_skip_cache_for_content(news_data, "æ–°é—»æ•°æ®"):
            # ç”Ÿæˆä¸€ä¸ªè™šæ‹Ÿçš„ç¼“å­˜é”®ï¼Œä½†ä¸å®é™…ä¿å­˜
            cache_key = self._generate_cache_key("news", symbol,
                                               start_date=start_date,
                                               end_date=end_date,
                                               source=data_source,
                                               skipped=True)
            logger.info(f"ğŸš« æ–°é—»æ•°æ®å› å†…å®¹è¿‡é•¿è¢«è·³è¿‡ç¼“å­˜: {symbol} -> {cache_key}")
            return cache_key

        cache_key = self._generate_cache_key("news", symbol,
                                           start_date=start_date,
                                           end_date=end_date,
                                           source=data_source)
        
        cache_path = self._get_cache_path("news", cache_key, "txt")
        cache_path.parent.mkdir(parents=True, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        with open(cache_path, 'w', encoding='utf-8') as f:
            f.write(news_data)
        
        metadata = {
            'symbol': symbol,
            'data_type': 'news',
            'start_date': start_date,
            'end_date': end_date,
            'data_source': data_source,
            'file_path': str(cache_path),
            'file_format': 'txt',
            'content_length': len(news_data)
        }
        self._save_metadata(cache_key, metadata)
        
        logger.info(f"ğŸ“° æ–°é—»æ•°æ®å·²ç¼“å­˜: {symbol} ({data_source}) -> {cache_key}")
        return cache_key
    
    def save_fundamentals_data(self, symbol: str, fundamentals_data: str,
                              data_source: str = "unknown") -> str:
        """ä¿å­˜åŸºæœ¬é¢æ•°æ®åˆ°ç¼“å­˜"""
        # æ£€æŸ¥å†…å®¹é•¿åº¦æ˜¯å¦éœ€è¦è·³è¿‡ç¼“å­˜
        if self.should_skip_cache_for_content(fundamentals_data, "åŸºæœ¬é¢æ•°æ®"):
            # ç”Ÿæˆä¸€ä¸ªè™šæ‹Ÿçš„ç¼“å­˜é”®ï¼Œä½†ä¸å®é™…ä¿å­˜
            market_type = self._determine_market_type(symbol)
            cache_key = self._generate_cache_key("fundamentals", symbol,
                                               source=data_source,
                                               market=market_type,
                                               date=datetime.now().strftime("%Y-%m-%d"),
                                               skipped=True)
            logger.info(f"ğŸš« åŸºæœ¬é¢æ•°æ®å› å†…å®¹è¿‡é•¿è¢«è·³è¿‡ç¼“å­˜: {symbol} -> {cache_key}")
            return cache_key

        market_type = self._determine_market_type(symbol)
        cache_key = self._generate_cache_key("fundamentals", symbol,
                                           source=data_source,
                                           market=market_type,
                                           date=datetime.now().strftime("%Y-%m-%d"))
        
        cache_path = self._get_cache_path("fundamentals", cache_key, "txt", symbol)
        cache_path.parent.mkdir(parents=True, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        with open(cache_path, 'w', encoding='utf-8') as f:
            f.write(fundamentals_data)
        
        metadata = {
            'symbol': symbol,
            'data_type': 'fundamentals',
            'data_source': data_source,
            'market_type': market_type,
            'file_path': str(cache_path),
            'file_format': 'txt',
            'content_length': len(fundamentals_data)
        }
        self._save_metadata(cache_key, metadata)
        
        desc = self.cache_config.get(f"{market_type}_fundamentals", {}).get('description', 'åŸºæœ¬é¢æ•°æ®')
        logger.info(f"ğŸ’¼ {desc}å·²ç¼“å­˜: {symbol} ({data_source}) -> {cache_key}")
        return cache_key
    
    def load_fundamentals_data(self, cache_key: str) -> Optional[str]:
        """ä»ç¼“å­˜åŠ è½½åŸºæœ¬é¢æ•°æ®"""
        metadata = self._load_metadata(cache_key)
        if not metadata:
            return None
        
        cache_path = Path(metadata['file_path'])
        if not cache_path.exists():
            return None
        
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            logger.error(f"âš ï¸ åŠ è½½åŸºæœ¬é¢ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
            return None
    
    def find_cached_fundamentals_data(self, symbol: str, data_source: str = None,
                                    max_age_hours: int = None) -> Optional[str]:
        """
        æŸ¥æ‰¾åŒ¹é…çš„åŸºæœ¬é¢ç¼“å­˜æ•°æ®
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data_source: æ•°æ®æºï¼ˆå¦‚ "openai", "finnhub"ï¼‰
            max_age_hours: æœ€å¤§ç¼“å­˜æ—¶é—´ï¼ˆå°æ—¶ï¼‰ï¼ŒNoneæ—¶ä½¿ç”¨æ™ºèƒ½é…ç½®
        
        Returns:
            cache_key: å¦‚æœæ‰¾åˆ°æœ‰æ•ˆç¼“å­˜åˆ™è¿”å›ç¼“å­˜é”®ï¼Œå¦åˆ™è¿”å›None
        """
        market_type = self._determine_market_type(symbol)
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šTTLï¼Œä½¿ç”¨æ™ºèƒ½é…ç½®
        if max_age_hours is None:
            cache_type = f"{market_type}_fundamentals"
            max_age_hours = self.cache_config.get(cache_type, {}).get('ttl_hours', 24)
        
        # æŸ¥æ‰¾åŒ¹é…çš„ç¼“å­˜
        for metadata_file in self.metadata_dir.glob(f"*_meta.json"):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                if (metadata.get('symbol') == symbol and
                    metadata.get('data_type') == 'fundamentals' and
                    metadata.get('market_type') == market_type and
                    (data_source is None or metadata.get('data_source') == data_source)):
                    
                    cache_key = metadata_file.stem.replace('_meta', '')
                    if self.is_cache_valid(cache_key, max_age_hours, symbol, 'fundamentals'):
                        desc = self.cache_config.get(f"{market_type}_fundamentals", {}).get('description', 'åŸºæœ¬é¢æ•°æ®')
                        logger.info(f"ğŸ¯ æ‰¾åˆ°åŒ¹é…çš„{desc}ç¼“å­˜: {symbol} ({data_source}) -> {cache_key}")
                        return cache_key
            except Exception:
                continue
        
        desc = self.cache_config.get(f"{market_type}_fundamentals", {}).get('description', 'åŸºæœ¬é¢æ•°æ®')
        logger.error(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„{desc}ç¼“å­˜: {symbol} ({data_source})")
        return None
    
    def clear_old_cache(self, max_age_days: int = 7):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        cutoff_time = datetime.now() - timedelta(days=max_age_days)
        cleared_count = 0
        
        for metadata_file in self.metadata_dir.glob("*_meta.json"):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                cached_at = datetime.fromisoformat(metadata['cached_at'])
                if cached_at < cutoff_time:
                    # åˆ é™¤æ•°æ®æ–‡ä»¶
                    data_file = Path(metadata['file_path'])
                    if data_file.exists():
                        data_file.unlink()
                    
                    # åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶
                    metadata_file.unlink()
                    cleared_count += 1
                    
            except Exception as e:
                logger.warning(f"âš ï¸ æ¸…ç†ç¼“å­˜æ—¶å‡ºé”™: {e}")
        
        logger.info(f"ğŸ§¹ å·²æ¸…ç† {cleared_count} ä¸ªè¿‡æœŸç¼“å­˜æ–‡ä»¶")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_files': 0,
            'stock_data_count': 0,
            'news_count': 0,
            'fundamentals_count': 0,
            'total_size_mb': 0,
            'skipped_count': 0  # æ–°å¢ï¼šè·³è¿‡çš„ç¼“å­˜æ•°é‡
        }
        
        for metadata_file in self.metadata_dir.glob("*_meta.json"):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                data_type = metadata.get('data_type', 'unknown')
                if data_type == 'stock_data':
                    stats['stock_data_count'] += 1
                elif data_type == 'news':
                    stats['news_count'] += 1
                elif data_type == 'fundamentals':
                    stats['fundamentals_count'] += 1
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºè·³è¿‡çš„ç¼“å­˜ï¼ˆæ²¡æœ‰å®é™…æ–‡ä»¶ï¼‰
                data_file = Path(metadata.get('file_path', ''))
                if not data_file.exists():
                    stats['skipped_count'] += 1
                else:
                    # è®¡ç®—æ–‡ä»¶å¤§å°
                    stats['total_size_mb'] += data_file.stat().st_size / (1024 * 1024)
                
                stats['total_files'] += 1
                
            except Exception:
                continue
        
        stats['total_size_mb'] = round(stats['total_size_mb'], 2)
        return stats

    def get_content_length_config_status(self) -> Dict[str, Any]:
        """è·å–å†…å®¹é•¿åº¦é…ç½®çŠ¶æ€"""
        available_providers = self._check_provider_availability()
        long_text_providers = self.content_length_config['long_text_providers']
        available_long_providers = [p for p in available_providers if p in long_text_providers]
        
        return {
            'enabled': self.content_length_config['enable_length_check'],
            'max_content_length': self.content_length_config['max_content_length'],
            'max_content_length_formatted': f"{self.content_length_config['max_content_length']:,}å­—ç¬¦",
            'long_text_providers': long_text_providers,
            'available_providers': available_providers,
            'available_long_providers': available_long_providers,
            'has_long_text_support': len(available_long_providers) > 0,
            'will_skip_long_content': self.content_length_config['enable_length_check'] and len(available_long_providers) == 0
        }


# å…¨å±€ç¼“å­˜å®ä¾‹
_cache_instance = None

def get_cache() -> StockDataCache:
    """è·å–å…¨å±€ç¼“å­˜å®ä¾‹"""
    global _cache_instance
    if _cache_instance is None:
        _cache_instance = StockDataCache()
    return _cache_instance
